import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import requests
import json
import tempfile
import os

# 1. T·∫£i model embedding
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer("all-MiniLM-L6-v2", device="cpu")

st.set_page_config(page_title="H·ªá th·ªëng H·ªèi ƒë√°p AI", layout="wide")
st.title("ü¶â H·ªá th·ªëng H·ªèi ƒë√°p Th√¥ng minh cho T√†i li·ªáu PDF ho·∫∑c Chatbot")

# 2. H√†m x·ª≠ l√Ω file PDF
def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getbuffer())
        return tmp_file.name

# 3. H√†m t·∫°o FAISS index
def create_faiss_index(embeddings):
    quantizer = faiss.IndexFlatL2(embeddings.shape[1])
    nlist = min(50, len(embeddings)) if len(embeddings) > 0 else 1
    if len(embeddings) < nlist:
        nlist = len(embeddings)
    if nlist < 1:
        nlist = 1
    index = faiss.IndexIVFFlat(quantizer, embeddings.shape[1], nlist)
    index.train(embeddings)
    index.add(embeddings)
    return index

# 4. H√†m g·ªçi LLM (n·∫øu c√≥ context th√¨ d√πng, kh√¥ng th√¨ h·ªèi t·ª± do)
def query_llm(context, query):
    if context:
        system_prompt = "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p."
        user_prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
    else:
        system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, h√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi m·ªôt c√°ch t·ª± nhi√™n v√† ch√≠nh x√°c."
        user_prompt = query

    payload = {
        'temperature': 0.1,
        'max_tokens': 512,
        'messages': [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    }
    try:
        response = requests.post(
            "http://localhost:1234/v1/chat/completions",
            json=payload,
            headers={'Content-Type': 'application/json'},
            timeout=30
        )
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except requests.exceptions.RequestException as e:
        st.error(f"L·ªói k·∫øt n·ªëi: {str(e)}")
        return None

# 5. Kh·ªüi t·∫°o model embedding
model = load_embedding_model()

# 6. Cho ph√©p upload file (ho·∫∑c kh√¥ng)
uploaded_file = st.file_uploader("üì§ T·∫£i l√™n file PDF (t√πy ch·ªçn)", type="pdf", accept_multiple_files=False)

# 7. Nh·∫≠p c√¢u h·ªèi (lu√¥n hi·ªÉn th·ªã, kh√¥ng ph·ª• thu·ªôc vi·ªác upload file)
query = st.text_input("üí¨ Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", placeholder="B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ t√†i li·ªáu ho·∫∑c h·ªèi b·∫•t k·ª≥ ƒëi·ªÅu g√¨...")

# 8. N·∫øu c√≥ file, x·ª≠ l√Ω file v√† t·∫°o index
faiss_index = None
texts = None
if uploaded_file:
    with st.spinner("üîç ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
        pdf_path = process_pdf(uploaded_file)
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        os.unlink(pdf_path)  # X√≥a file t·∫°m sau khi ƒë·ªçc xong
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            separators=["\n\n", "\n", " ", ""]
        )
        texts = text_splitter.split_documents(docs)
        if len(texts) > 0:
            text_embeddings = model.encode([text.page_content for text in texts], show_progress_bar=False)
            faiss_index = create_faiss_index(text_embeddings)

# 9. Khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi
if query:
    with st.spinner("ü§ñ ƒêang ph√¢n t√≠ch..."):
        context = ""
        # N·∫øu ƒë√£ upload file v√† ƒë√£ t·∫°o index, th·ª±c hi·ªán RAG
        if uploaded_file and faiss_index is not None and texts is not None and len(texts) > 0:
            query_embedding = model.encode(query, show_progress_bar=False)
            D, I = faiss_index.search(np.array([query_embedding]), k=min(3, len(texts)))
            context = "\n---\n".join([texts[i].page_content for i in I[0]])
        # N·∫øu kh√¥ng c√≥ file, context ƒë·ªÉ tr·ªëng, AI s·∫Ω tr·∫£ l·ªùi t·ª± do
        answer = query_llm(context, query)
        if answer:
            st.subheader("üìù C√¢u tr·∫£ l·ªùi:")
            st.success(answer)
            if context:
                with st.expander("Xem ng·ªØ c·∫£nh tham kh·∫£o"):
                    st.markdown(context)
        else:
            st.warning("Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p.")
